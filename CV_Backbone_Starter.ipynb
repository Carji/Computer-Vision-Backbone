{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install mediapipe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install opencv-python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#MediaPipe\r\n",
    "\r\n",
    "MediaPipe offers open source cross-platform, customizable ML solutions for live and streaming media. \r\n",
    "End-to-End acceleration: Built-in fast ML inference and processing accelerated even on common hardware. \r\n",
    "Build once, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#face mesh 3d with body pose on images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import glob\r\n",
    "\r\n",
    "# Read images with OpenCV.\r\n",
    "images = {name: cv2.imread(name) for name in glob.glob(\"./images/testing/*.jpg\")}"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, image in images.items():\r\n",
    "    image =cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "#    plt.imshow(image)\r\n",
    "#    plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mediapipe as mp\r\n",
    "mp_holistic = mp.solutions.holistic\r\n",
    "\r\n",
    "#help(mp_holistic.Holistic)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare DrawingSpec for drawing the face landmarks later.\r\n",
    "mp_drawing = mp.solutions.drawing_utils \r\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize MediaPipe Pose.\r\n",
    "import cv2\r\n",
    "import time\r\n",
    "import mediapipe as mp\r\n",
    "\r\n",
    "\r\n",
    "class PoseDetector:\r\n",
    "\r\n",
    "    def __init__(self, mode = False, upBody = False, smooth=True, detectionCon = 0.5, trackCon = 0.5):\r\n",
    "\r\n",
    "        self.mode = mode\r\n",
    "        self.upBody = upBody\r\n",
    "        self.smooth = smooth\r\n",
    "        self.detectionCon = detectionCon\r\n",
    "        self.trackCon = trackCon\r\n",
    "\r\n",
    "        self.mpDraw = mp.solutions.drawing_utils\r\n",
    "        self.mpPose = mp.solutions.pose\r\n",
    "        self.pose = self.mpPose.Pose(self.mode, self.upBody, self.smooth, self.detectionCon, self.trackCon)\r\n",
    "\r\n",
    "    def findPose(self, img, draw=True):\r\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
    "        self.results = self.pose.process(imgRGB)\r\n",
    "        if self.results.pose_landmarks:\r\n",
    "            if draw:\r\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\r\n",
    "\r\n",
    "        return img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS\r\n",
    "\r\n",
    "    def getPosition(self, img, draw=True):\r\n",
    "        lmList= []\r\n",
    "        if self.results.pose_landmarks:\r\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\r\n",
    "                h, w, c = img.shape\r\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\r\n",
    "                lmList.append([id, cx, cy])\r\n",
    "                if draw:\r\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\r\n",
    "        return lmList\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "pTime = 0\r\n",
    "cap = cv2.VideoCapture(0)\r\n",
    "\r\n",
    "detector = PoseDetector()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "i=0\r\n",
    "img_array = []\r\n",
    "\r\n",
    "for name, image in images.items():\r\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\r\n",
    "\r\n",
    "\r\n",
    "    img, p_landmarks, p_connections = detector.findPose(image, False)\r\n",
    "    \r\n",
    "    \r\n",
    "    # draw points\r\n",
    "    mp.solutions.drawing_utils.draw_landmarks(img, p_landmarks, p_connections)\r\n",
    "    lmList = detector.getPosition(img)\r\n",
    "#    plt.imshow(annotated_image)\r\n",
    "#    plt.show()\r\n",
    "    height, width, layers = img.shape\r\n",
    "    size = (width,height)\r\n",
    "    img_array.append(img)\r\n",
    " \r\n",
    " \r\n",
    "out = cv2.VideoWriter('projectYOGA-POSE.avi',cv2.VideoWriter_fourcc(*'DIVX'), 24, size)\r\n",
    " \r\n",
    "for i in range(len(img_array)):\r\n",
    "    out.write(img_array[i])\r\n",
    "out.release()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize MediaPipe Holistic.\r\n",
    "i=0\r\n",
    "img_array = []\r\n",
    "with mp_holistic.Holistic(\r\n",
    "    static_image_mode=False,min_detection_confidence=0.8) as holistic:\r\n",
    "  for name, image in images.items():\r\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\r\n",
    "    results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n",
    "\r\n",
    "    # Print nose coordinates.\r\n",
    "    image_hight, image_width, _ = image.shape\r\n",
    "    if results.pose_landmarks:\r\n",
    "      print(\r\n",
    "        f'Nose coordinates: ('\r\n",
    "        f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '\r\n",
    "        f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_hight})'\r\n",
    "      )\r\n",
    "    # Draw pose landmarks.\r\n",
    "    print(f'Pose landmarks of {name}:')\r\n",
    "    annotated_image = image.copy()\r\n",
    "    mp_drawing.draw_landmarks(annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\r\n",
    "    mp_drawing.draw_landmarks(annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\r\n",
    "    mp_drawing.draw_landmarks(\r\n",
    "        image=annotated_image, \r\n",
    "        landmark_list=results.face_landmarks, \r\n",
    "        connections=mp_holistic.FACE_CONNECTIONS,\r\n",
    "        landmark_drawing_spec=drawing_spec,\r\n",
    "        connection_drawing_spec=drawing_spec)\r\n",
    "    mp_drawing.draw_landmarks(\r\n",
    "        image=annotated_image, \r\n",
    "        landmark_list=results.pose_landmarks, \r\n",
    "        connections=mp_holistic.POSE_CONNECTIONS,\r\n",
    "        landmark_drawing_spec=drawing_spec,\r\n",
    "        connection_drawing_spec=drawing_spec)\r\n",
    "    #cv2_imshow(annotated_image)\r\n",
    "    #cv2.imwrite(\"imag1\", annotated_image)\r\n",
    "    plt.figure(figsize=(10,15))\r\n",
    "#    plt.imshow(annotated_image)\r\n",
    "#    plt.show()\r\n",
    "    height, width, layers = image.shape\r\n",
    "    size = (width,height)\r\n",
    "    img_array.append(annotated_image)\r\n",
    " \r\n",
    " \r\n",
    "out = cv2.VideoWriter('projectYOGA.avi',cv2.VideoWriter_fourcc(*'DIVX'), 24, size)\r\n",
    " \r\n",
    "for i in range(len(img_array)):\r\n",
    "    out.write(img_array[i])\r\n",
    "out.release()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from abc import ABCMeta, abstractmethod\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "class AbstDetector(metaclass=ABCMeta):\r\n",
    "    @abstractmethod\r\n",
    "    def detect(self, image: np.ndarray) -> bool:\r\n",
    "        \"\"\"モデルによる推論処理\r\n",
    "        Args:\r\n",
    "            image (np.ndarray): 入力イメージ\r\n",
    "        Returns:\r\n",
    "            bool: 対象物体（手や顔）が検出できたかどうか\r\n",
    "        \"\"\"\r\n",
    "        pass\r\n",
    "\r\n",
    "    @abstractmethod\r\n",
    "    def draw(self, image: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"推論結果を描画する\r\n",
    "        Args:\r\n",
    "            image (np.ndarray): ベースイメージ\r\n",
    "        Returns:\r\n",
    "            np.ndarray: 描画済みイメージ\r\n",
    "        \"\"\"\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import mediapipe as mp\r\n",
    "\r\n",
    "\r\n",
    "from .abst_detector import AbstDetector\r\n",
    "\r\n",
    "\r\n",
    "class PoseEstimator(AbstDetector):\r\n",
    "    def __init__(self, min_detection_confidence: float, min_tracking_confidence: float) -> None:\r\n",
    "        \"\"\"初期化処理\r\n",
    "        Args:\r\n",
    "            min_detection_confidence (float): 姿勢推定モデルの最小信頼値\r\n",
    "            min_tracking_confidence (float): ランドマーク追跡モデルからの最小信頼値\r\n",
    "        \"\"\"\r\n",
    "        self.estimator = mp.solutions.pose.Pose(\r\n",
    "            min_detection_confidence=min_detection_confidence,\r\n",
    "            min_tracking_confidence=min_tracking_confidence,\r\n",
    "        )\r\n",
    "\r\n",
    "    def detect(self, image: np.ndarray) -> bool:\r\n",
    "        \"\"\"姿勢推定処理\r\n",
    "        Args:\r\n",
    "            image (np.ndarray): 入力イメージ\r\n",
    "        Returns:\r\n",
    "            bool: 手が検出できたか\r\n",
    "        \"\"\"\r\n",
    "        try:\r\n",
    "            self.results = self.estimator.process(image)\r\n",
    "        except Exception as e:\r\n",
    "            logger.error(e)\r\n",
    "        return True if self.results.pose_landmarks is not None else False\r\n",
    "\r\n",
    "    def draw(self, image: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"処理結果を描画する\r\n",
    "        Args:\r\n",
    "            image (np.ndarray): ベースイメージ\r\n",
    "        Returns:\r\n",
    "            np.ndarray: 描画済みイメージ\r\n",
    "        \"\"\"\r\n",
    "        landmark_buf = []\r\n",
    "        base_width, base_height = image.shape[1], image.shape[0]\r\n",
    "\r\n",
    "        # draw landmark points\r\n",
    "        for landmark in self.results.pose_landmarks.landmark:\r\n",
    "            x = min(int(landmark.x * base_width), base_width - 1)\r\n",
    "            y = min(int(landmark.y * base_height), base_height - 1)\r\n",
    "            landmark_buf.append((x, y))\r\n",
    "            cv2.circle(image, (x, y), 2, (255, 0, 0), 5)\r\n",
    "\r\n",
    "        # draw connections\r\n",
    "        for con_pair in mp.solutions.pose.POSE_CONNECTIONS:\r\n",
    "            cv2.line(image, landmark_buf[con_pair[0].value],\r\n",
    "                     landmark_buf[con_pair[1].value], (255, 0, 0), 2)\r\n",
    "\r\n",
    "        return image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#face mesh 3d on camera"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install mediapipe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install opencv-python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Face Mesh 468 point 3D face landmarks in video even on mobile devices and web application MediaPipe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#camera"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#image manipulation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\r\n",
    "\r\n",
    "import cv2\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "#Import image\r\n",
    "image = cv2.imread(\"image1073.jpg\")\r\n",
    "gray =cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "#Show the image with matplotlib\r\n",
    "plt.imshow(gray)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import matplotlib \r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "# Acquire default dots per inch value of matplotlib\r\n",
    "dpi = matplotlib.rcParams['figure.dpi']\r\n",
    "\r\n",
    "img = cv2.imread(\"image1073.jpg\")\r\n",
    "img =cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\r\n",
    "\r\n",
    "# Determine the figures size in inches to fit your image\r\n",
    "height, width, depth = img.shape\r\n",
    "figsize = width / float(dpi), height / float(dpi)\r\n",
    "\r\n",
    "plt.figure(figsize=figsize)\r\n",
    "plt.imshow(img)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "figsize"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}