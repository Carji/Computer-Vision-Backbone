{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaPipe\n",
    "\n",
    "MediaPipe offers open source cross-platform, customizable ML solutions for live and streaming media. \n",
    "End-to-End acceleration: Built-in fast ML inference and processing accelerated even on common hardware. \n",
    "Build once, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face mesh 3d with body pose on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Read images with OpenCV.\n",
    "images = {name: cv2.imread(name) for name in glob.glob(\"./images/testing/*.jpg\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, image in images.items():\n",
    "    image =cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#    plt.imshow(image)\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "#help(mp_holistic.Holistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_bool(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: bool) -> mediapipe.python._framework_bindings.packet.Packet\n\nInvoked with: 0.5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7628/1470074599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPoseDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7628/1470074599.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mode, upBody, smooth, detectionCon, trackCon)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpDraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawing_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpPose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpPose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupBody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectionCon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackCon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfindPose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, static_image_mode, model_complexity, smooth_landmarks, enable_segmentation, smooth_segmentation, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m    145\u001b[0m     \u001b[0m_download_oss_pose_landmark_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_complexity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     super().__init__(\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0mbinary_graph_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBINARYPB_FILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         side_inputs={\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, binary_graph_path, graph_config, calculator_params, side_inputs, outputs)\u001b[0m\n\u001b[0;32m    256\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve_output_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     self._input_side_packets = {\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_packet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_side_input_type_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mside_inputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     self._input_side_packets = {\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_packet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_side_input_type_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mside_inputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     }\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36m_make_packet\u001b[1;34m(self, packet_data_type, data)\u001b[0m\n\u001b[0;32m    511\u001b[0m           data, image_format=image_frame.ImageFormat.SRGB)\n\u001b[0;32m    512\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacket_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'create_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpacket_data_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m   def _get_packet_content(self, packet_data_type: _PacketDataType,\n",
      "\u001b[1;31mTypeError\u001b[0m: create_bool(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: bool) -> mediapipe.python._framework_bindings.packet.Packet\n\nInvoked with: 0.5"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Pose.\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "class PoseDetector:\n",
    "\n",
    "    def __init__(self, mode = False, upBody = False, smooth=True, detectionCon = 0.5, trackCon = 0.5):\n",
    "\n",
    "        self.mode = mode\n",
    "        self.upBody = upBody\n",
    "        self.smooth = smooth\n",
    "        self.detectionCon = detectionCon\n",
    "        self.trackCon = trackCon\n",
    "\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(self.mode, self.upBody, self.smooth, self.detectionCon, self.trackCon)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks:\n",
    "            if draw:\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\n",
    "\n",
    "        return img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS\n",
    "\n",
    "    def getPosition(self, img, draw=True):\n",
    "        lmList= []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\n",
    "        return lmList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pTime = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "detector = PoseDetector()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "img_array = []\n",
    "\n",
    "for name, image in images.items():\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
    "\n",
    "\n",
    "    img, p_landmarks, p_connections = detector.findPose(image, False)\n",
    "    \n",
    "    \n",
    "    # draw points\n",
    "    mp.solutions.drawing_utils.draw_landmarks(img, p_landmarks, p_connections)\n",
    "    lmList = detector.getPosition(img)\n",
    "#    plt.imshow(annotated_image)\n",
    "#    plt.show()\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    " \n",
    " \n",
    "out = cv2.VideoWriter('projectYOGA-POSE.avi',cv2.VideoWriter_fourcc(*'DIVX'), 24, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic.\n",
    "i=0\n",
    "img_array = []\n",
    "with mp_holistic.Holistic(\n",
    "    static_image_mode=False,min_detection_confidence=0.8) as holistic:\n",
    "  for name, image in images.items():\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
    "    results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print nose coordinates.\n",
    "    image_hight, image_width, _ = image.shape\n",
    "    if results.pose_landmarks:\n",
    "      print(\n",
    "        f'Nose coordinates: ('\n",
    "        f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '\n",
    "        f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_hight})'\n",
    "      )\n",
    "    # Draw pose landmarks.\n",
    "    print(f'Pose landmarks of {name}:')\n",
    "    annotated_image = image.copy()\n",
    "    mp_drawing.draw_landmarks(annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image, \n",
    "        landmark_list=results.face_landmarks, \n",
    "        connections=mp_holistic.FACE_CONNECTIONS,\n",
    "        landmark_drawing_spec=drawing_spec,\n",
    "        connection_drawing_spec=drawing_spec)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image, \n",
    "        landmark_list=results.pose_landmarks, \n",
    "        connections=mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=drawing_spec,\n",
    "        connection_drawing_spec=drawing_spec)\n",
    "    #cv2_imshow(annotated_image)\n",
    "    #cv2.imwrite(\"imag1\", annotated_image)\n",
    "    plt.figure(figsize=(10,15))\n",
    "#    plt.imshow(annotated_image)\n",
    "#    plt.show()\n",
    "    height, width, layers = image.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(annotated_image)\n",
    " \n",
    " \n",
    "out = cv2.VideoWriter('projectYOGA.avi',cv2.VideoWriter_fourcc(*'DIVX'), 24, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstDetector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def detect(self, image: np.ndarray) -> bool:\n",
    "        \"\"\"モデルによる推論処理\n",
    "        Args:\n",
    "            image (np.ndarray): 入力イメージ\n",
    "        Returns:\n",
    "            bool: 対象物体（手や顔）が検出できたかどうか\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"推論結果を描画する\n",
    "        Args:\n",
    "            image (np.ndarray): ベースイメージ\n",
    "        Returns:\n",
    "            np.ndarray: 描画済みイメージ\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "from .abst_detector import AbstDetector\n",
    "\n",
    "\n",
    "class PoseEstimator(AbstDetector):\n",
    "    def __init__(self, min_detection_confidence: float, min_tracking_confidence: float) -> None:\n",
    "        \"\"\"初期化処理\n",
    "        Args:\n",
    "            min_detection_confidence (float): 姿勢推定モデルの最小信頼値\n",
    "            min_tracking_confidence (float): ランドマーク追跡モデルからの最小信頼値\n",
    "        \"\"\"\n",
    "        self.estimator = mp.solutions.pose.Pose(\n",
    "            min_detection_confidence=min_detection_confidence,\n",
    "            min_tracking_confidence=min_tracking_confidence,\n",
    "        )\n",
    "\n",
    "    def detect(self, image: np.ndarray) -> bool:\n",
    "        \"\"\"姿勢推定処理\n",
    "        Args:\n",
    "            image (np.ndarray): 入力イメージ\n",
    "        Returns:\n",
    "            bool: 手が検出できたか\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.results = self.estimator.process(image)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "        return True if self.results.pose_landmarks is not None else False\n",
    "\n",
    "    def draw(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"処理結果を描画する\n",
    "        Args:\n",
    "            image (np.ndarray): ベースイメージ\n",
    "        Returns:\n",
    "            np.ndarray: 描画済みイメージ\n",
    "        \"\"\"\n",
    "        landmark_buf = []\n",
    "        base_width, base_height = image.shape[1], image.shape[0]\n",
    "\n",
    "        # draw landmark points\n",
    "        for landmark in self.results.pose_landmarks.landmark:\n",
    "            x = min(int(landmark.x * base_width), base_width - 1)\n",
    "            y = min(int(landmark.y * base_height), base_height - 1)\n",
    "            landmark_buf.append((x, y))\n",
    "            cv2.circle(image, (x, y), 2, (255, 0, 0), 5)\n",
    "\n",
    "        # draw connections\n",
    "        for con_pair in mp.solutions.pose.POSE_CONNECTIONS:\n",
    "            cv2.line(image, landmark_buf[con_pair[0].value],\n",
    "                     landmark_buf[con_pair[1].value], (255, 0, 0), 2)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#face mesh 3d on camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Face Mesh 468 point 3D face landmarks in video even on mobile devices and web application MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Import image\n",
    "image = cv2.imread(\"image1073.jpg\")\n",
    "gray =cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#Show the image with matplotlib\n",
    "plt.imshow(gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# Acquire default dots per inch value of matplotlib\n",
    "dpi = matplotlib.rcParams['figure.dpi']\n",
    "\n",
    "img = cv2.imread(\"image1073.jpg\")\n",
    "img =cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Determine the figures size in inches to fit your image\n",
    "height, width, depth = img.shape\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ccc2e60edeb4d498c2b0acab7ca2dcfc0a3e4f99264e4d9040c131c36322ec9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
